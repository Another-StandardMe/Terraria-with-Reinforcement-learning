import os
import math
import torch
import random
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Categorical
from G_PolicyNet import GTrXLPolicyNet, orthogonal_init
from G_ValueNet import GTrXLValueNet, orthogonal_init

class PPO:
    def __init__(self, args):
        self.device = args.device
        self.gamma = args.gamma
        self.epsilon = args.epsilon
        self.K_epochs = args.K_epochs
        self.entropy_coef = args.entropy_coef
        self.entropy_coef_max = args.entropy_coef
        self.clip_value = args.clip_value
        self.use_grad_clip = getattr(args, "use_grad_clip", False)
        self.use_lr_decay = getattr(args, "use_lr_decay", False)
        self.lamda = args.lamda
        self.max_episodes = args.max_episodes
        self.num_critic_updates = 5
        self.count = 0
        self.trajectory_count = 0
        # æ–°å¢è®°å½•å„ episode æŒ‡æ ‡çš„å±æ€§
        self.ep_avg_losses = []  # ç”¨äºè®°å½•æ¯ä¸ª episode çš„å¹³å‡ç­–ç•¥ lossï¼ˆä¹Ÿå¯åŒ…å« critic lossï¼‰
        self.ep_avg_qs = []      # ç”¨äºè®°å½•æ¯ä¸ª episode çš„å¹³å‡ Q å€¼ï¼ˆCritic è¾“å‡ºå‡å€¼ï¼‰
        self.ep_lengths = []     # è®°å½•æ¯ä¸ª episode çš„æ­¥æ•°ï¼ˆå¦‚æœåœ¨ Worker ä¸­ç»Ÿè®¡ï¼Œå¯ä»¥åœ¨æ­¤æ›´æ–°ï¼‰
        self.ep_rewards = []     # è®°å½•æ¯ä¸ª episode çš„æ€»å¥–åŠ±ï¼ˆå¦‚æœåœ¨ Worker ä¸­ç»Ÿè®¡ï¼Œå¯ä»¥åœ¨æ­¤æ›´æ–°ï¼‰

        # **åˆ›å»º Actor å’Œ Critic**
        self.actor = GTrXLPolicyNet(
            img_feature_dim=args.img_feature_dim,
            transformer_dim=args.transformer_dim,
            hidden_dim=args.hidden_width,
            transformer_heads=args.transformer_heads,
            transformer_layers=args.transformer_layers,
            dropout_rate=args.dropout_rate,
        ).to(self.device)

        self.critic = GTrXLValueNet(
            img_feature_dim=args.img_feature_dim,
            transformer_dim=args.transformer_dim,
            hidden_dim=args.hidden_width,
            transformer_heads=args.transformer_heads,
            transformer_layers=args.transformer_layers,
            dropout_rate=args.dropout_rate,
        ).to(self.device)

        # **ğŸš€ æ‰§è¡Œæ­£äº¤åˆå§‹åŒ–**
        self._initialize_weights()
        self.set_seed(42)

        if getattr(args, "set_adam_eps", False):
            self.optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr=args.lr_a, eps=1e-5)
            self.optimizer_critic = torch.optim.Adam(self.critic.parameters(), lr=args.lr_c, eps=1e-5)
        else:
            self.optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr=args.lr_a)
            self.optimizer_critic = torch.optim.Adam(self.critic.parameters(), lr=args.lr_c)

        self.checkpoint_path = "D:/RL_Terraria/Project_TAI/newest/checkpoints/Terraria_final_model_005.pth"
        if os.path.exists(self.checkpoint_path):
            print(f"ğŸ”„ åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹: {self.checkpoint_path}")
            self.load(self.checkpoint_path)
            # self.load_actor(self.checkpoint_path)
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°å·²ä¿å­˜çš„æ¨¡å‹ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–å‚æ•°")

        # âœ… æ³¨æ„ï¼šæŠŠ scheduler åˆå§‹åŒ–æ”¾åˆ° load() ä¹‹å
        self.scheduler_actor = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer_actor, T_max=self.max_episodes, eta_min=1e-5)
        self.scheduler_critic = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer_critic, T_max=self.max_episodes, eta_min=3e-5)

        # self.scheduler_actor = torch.optim.lr_scheduler.CosineAnnealingLR(
        #     self.optimizer_actor, T_max=args.max_train_steps, eta_min=1e-4)
        # self.scheduler_critic = torch.optim.lr_scheduler.CosineAnnealingLR(
        #     self.optimizer_critic, T_max=args.max_train_steps, eta_min=5e-5)
        #
        # self.checkpoint_path = "checkpoints/Terraria_final_model_003.pth"
        # if os.path.exists(self.checkpoint_path):
        #     print(f"ğŸ”„ åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹: {self.checkpoint_path}")
        #     self.load(self.checkpoint_path)
        #     print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        # else:
        #     print("âš ï¸ æœªæ‰¾åˆ°å·²ä¿å­˜çš„æ¨¡å‹ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–å‚æ•°")

    def _initialize_weights(self):
        """
        ğŸš€ **ä½¿ç”¨æ­£äº¤åˆå§‹åŒ–å¯¹ Actor å’Œ Critic ç½‘ç»œè¿›è¡Œæƒé‡åˆå§‹åŒ–**
        """
        self.actor.apply(orthogonal_init)
        self.critic.apply(orthogonal_init)
        print("âœ… æ­£äº¤åˆå§‹åŒ–å®Œæˆ: Actor & Critic")

    @staticmethod  # âœ… æ·»åŠ è¿™ä¸ªè£…é¥°å™¨ï¼Œä½¿å…¶æˆä¸ºé™æ€æ–¹æ³•
    def set_seed(seed=42):
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    def get_action(self, state):
        """ é‡‡æ ·åŠ¨ä½œå¹¶è¿”å›å¤„ç†åçš„ Python åŸç”Ÿç±»å‹ """
        state_tensor = torch.as_tensor(state, device=self.device, dtype=torch.float32)
        with torch.no_grad():
            actions, log_probs = self.actor.sample_action(state_tensor)

        return actions.cpu().numpy().squeeze(0).tolist(), log_probs.cpu().numpy().tolist()

    def update(self, replay_buffer):
        """
        æ›´æ–° PPO ç­–ç•¥å’Œ Critic
        replay_buffer ä¸­åŒ…å«ä¸€ä¸ªå•ç‹¬æ ·æœ¬ï¼š (s, a, a_logprob, r, s_, done)
        """
        s, a, a_logprob, r, s_, done = replay_buffer
        s = s.to(self.device)
        a = a.to(self.device)
        a_logprob = a_logprob.to(self.device)
        r = r.to(self.device)
        s_ = s_.to(self.device)
        done = done.to(self.device)

        # è®¡ç®— TD(0) ç›®æ ‡
        with torch.no_grad():
            v_s_ = self.critic(s_, update_memory=False).squeeze(-1)
            v_target = r + self.gamma * (1 - done) * v_s_

        # k_epochs å®é™…ä¸Šæ˜¯å†³å®šä½ è¦åœ¨åŒä¸€ä¸ªçŠ¶æ€ä¸Šå°è¯•å¤šå°‘æ¬¡åŠ¨ä½œï¼Œæ‰èƒ½è®©ç†µåå¡Œ
        for epoch in range(self.K_epochs):
            update_memory = (epoch == 0)  # åªæœ‰ç¬¬ä¸€ä¸ª epoch æ›´æ–° memory
            # è°ƒç”¨ actor æ—¶ä¼ é€’ update_memory å‚æ•°
            move_dist, jump_dist = self.actor.get_dist(s, update_memory=update_memory)

            # è°ƒç”¨ critic æ—¶ä¼ é€’ update_memory å‚æ•°
            v_s = self.critic(s, update_memory=update_memory).squeeze(-1)

            advantage = (v_target - v_s).detach()
            # è®¡ç®— PPO æŸå¤±
            ratio = torch.exp((move_dist.log_prob(a[:, 0]) + jump_dist.log_prob(a[:, 1])).view(-1, 1) - a_logprob)
            surr1 = ratio * advantage
            surr2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantage
            policy_loss = -torch.min(surr1, surr2).mean() - self.entropy_coef * (
                        move_dist.entropy().mean() + jump_dist.entropy().mean())

            values_clipped = v_s + (v_s - v_s.detach()).clamp(-self.clip_value, self.clip_value)
            value_loss_unclipped = (v_target - v_s).pow(2)
            value_loss_clipped = (v_target - values_clipped).pow(2)
            critic_loss = 0.5 * torch.max(value_loss_unclipped, value_loss_clipped).mean()

            # æ›´æ–° Actor
            self.optimizer_actor.zero_grad()
            policy_loss.backward()
            if self.use_grad_clip:
                nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
            self.optimizer_actor.step()

            # æ›´æ–° Critic
            self.optimizer_critic.zero_grad()
            critic_loss.backward()
            if self.use_grad_clip:
                nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)
            self.optimizer_critic.step()

        # if self.use_lr_decay:
        #     self.scheduler_actor.step()
        #     self.scheduler_critic.step()

        print(f"ğŸ’  ç¬¬[{self.count}]æ¬¡ æ›´æ–°......")
        self.count += 1

    def redistribute_final_reward(self, rewards, final_reward):
        """
        å°† final_reward åˆç†åœ°åˆ†é…åˆ° rewards[:-1] ä¸Šï¼Œå¹¶ç»™æœ€åä¸€ä¸ªçŠ¶æ€è®¾ç½® Â±50 çš„å¥–åŠ±
        æ»¡è¶³ï¼š
        1. åˆ†é…æ€»å’Œä¸º final_reward
        2. æœ€åçŠ¶æ€å†é¢å¤–è¡¥å…… Â±50 ç”¨ä½œ terminal signal
        """
        eps = 3
        r_tensor = torch.stack(rewards).squeeze()
        r_main = r_tensor[:-1]  # å‰ T ä¸ªçŠ¶æ€
        T = len(r_main)

        # æ„å»ºåˆ†é…æƒé‡
        if final_reward >= 0:
            min_r = r_main.min()
            weights = torch.where(r_main > 0, r_main - min_r + eps, torch.full_like(r_main, eps))
        else:
            max_r = r_main.max()
            weights = torch.where(r_main < 0, -r_main + max_r + eps, torch.full_like(r_main, eps))

        # åˆ†é… final_reward åˆ°å‰ T ä¸ªçŠ¶æ€
        weights = weights / (weights.sum() + eps)
        redistribution = weights * final_reward
        adjusted = r_main + redistribution

        # âœ… æœ€åä¸€ä¸ªçŠ¶æ€è®¾å®šä¸º Â±10ï¼ˆé¢å¤–å¥–åŠ±ï¼Œä¸å±äºåˆ†é…ï¼‰
        terminal_bonus = torch.tensor(10.0 if final_reward >= 0 else -10.0, device=r_tensor.device)
        adjusted_rewards = list(adjusted) + [terminal_bonus]

        return [r.unsqueeze(0) for r in adjusted_rewards]

    def update_trajectory(self, trajectory):
        states = torch.stack(trajectory["states"]).squeeze(1).to(self.device)
        actions = torch.stack(trajectory["actions"]).squeeze(1).to(self.device)
        log_probs = torch.stack(trajectory["log_probs"]).squeeze(1).to(self.device)
        rewards = trajectory["rewards"]
        next_states = torch.stack(trajectory["next_states"]).squeeze(1).to(self.device)
        dones = torch.stack(trajectory["dones"]).squeeze(1).to(self.device)

        final_reward = rewards[-1].item()
        redistributed_rewards = self.redistribute_final_reward(rewards, final_reward)
        rewards_tensor = torch.stack(redistributed_rewards).squeeze(1).to(self.device)

        print(" --------------1-----------------")
        print(f"[æ˜¾å­˜ç›‘æ§] å·²ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MB")
        print(f"        ä¿ç•™ç¼“å­˜: {torch.cuda.memory_reserved() / 1024 ** 2:.2f} MB")
        with torch.no_grad():
            self.critic.reset_memory()
            values = []
            for s in states:
                v = self.critic(s.unsqueeze(0), update_memory=True).squeeze()
                values.append(v)
            values = torch.stack(values)

            self.critic.reset_memory()
            next_values = []
            for s_ in next_states:
                v_ = self.critic(s_.unsqueeze(0), update_memory=True).squeeze()
                next_values.append(v_)
            next_values = torch.stack(next_values)

            advantages = torch.zeros_like(rewards_tensor).to(self.device)
            gae = 0
            for t in reversed(range(len(rewards_tensor))):
                next_v = (1 - dones[t]) * next_values[t]
                delta = rewards_tensor[t] + self.gamma * next_v - values[t]
                gae = delta + self.gamma * self.lamda * (1 - dones[t]) * gae
                advantages[t] = gae

            returns = advantages + values
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

            with open("gae_stats_log.txt", "a") as log_file:
                log_file.write(f"Advantages mean: {advantages.mean().item():.4f}, std: {advantages.std().item():.4f}\n")
                log_file.write(f"Returns mean: {returns.mean().item():.4f}, value mean: {values.mean().item():.4f}\n")

        print(" --------------2-----------------")
        print(f"[æ˜¾å­˜ç›‘æ§] å·²ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MB")
        print(f"        ä¿ç•™ç¼“å­˜: {torch.cuda.memory_reserved() / 1024 ** 2:.2f} MB")

        for epoch in range(self.K_epochs):
            print(f"ğŸŸ¢ è½¨è¿¹å¼åºåˆ—æ›´æ–°: ç¬¬ {epoch + 1}/{self.K_epochs} è½®")

            self.actor.reset_memory()
            self.critic.reset_memory()

            # ç´¯ç§¯æ—¶åºæ€§å¤„ç†ï¼šé€å¸§ forwardï¼Œæ‰¹é‡è®°å½• logits å’Œ values
            move_logits_list, jump_logits_list, value_preds = [], [], []
            for s in states:
                move_logits, jump_logits = self.actor.forward(s.unsqueeze(0), update_memory=True)
                v_s = self.critic(s.unsqueeze(0), update_memory=True).squeeze()
                move_logits_list.append(move_logits.squeeze(0))
                jump_logits_list.append(jump_logits.squeeze(0))
                value_preds.append(v_s)

            move_logits = torch.stack(move_logits_list)
            jump_logits = torch.stack(jump_logits_list)
            value_preds = torch.stack(value_preds)

            move_dist = Categorical(logits=move_logits)
            jump_dist = Categorical(logits=jump_logits)
            logp_new = move_dist.log_prob(actions[:, 0]) + jump_dist.log_prob(actions[:, 1])
            ratio = torch.exp(logp_new - log_probs.view(-1))

            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantages
            policy_loss = -torch.min(surr1, surr2).mean()
            entropy = move_dist.entropy().mean() + jump_dist.entropy().mean()
            policy_loss -= self.entropy_coef * entropy

            values_clipped = value_preds + (value_preds - value_preds.detach()).clamp(-self.clip_value, self.clip_value)
            value_loss_unclipped = (returns - value_preds).pow(2)
            value_loss_clipped = (returns - values_clipped).pow(2)
            critic_loss = 0.5 * torch.max(value_loss_unclipped, value_loss_clipped).mean()

            print(" --------------3-----------------")
            print(f"[æ˜¾å­˜ç›‘æ§] å·²ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MB")
            print(f"        ä¿ç•™ç¼“å­˜: {torch.cuda.memory_reserved() / 1024 ** 2:.2f} MB")
            # âœ… ä¸€æ¬¡æ€§åå‘ä¼ æ’­
            self.optimizer_actor.zero_grad()
            policy_loss.backward()
            if self.use_grad_clip:
                nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
            self.optimizer_actor.step()

            self.optimizer_critic.zero_grad()
            critic_loss.backward()
            if self.use_grad_clip:
                nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)
            self.optimizer_critic.step()

            for _ in range(self.num_critic_updates - 1):  # æ³¨æ„ -1ï¼Œå› ä¸ºä¸Šé¢å·²ç»æ›´æ–°è¿‡ä¸€æ¬¡äº†
                self.critic.reset_memory()
                value_preds_extra = []
                for s in states:
                    v_s = self.critic(s.unsqueeze(0), update_memory=True).squeeze()
                    value_preds_extra.append(v_s)
                value_preds_extra = torch.stack(value_preds_extra)

                values_clipped = value_preds_extra + (value_preds_extra - value_preds_extra.detach()).clamp(
                    -self.clip_value, self.clip_value)
                value_loss_unclipped = (returns - value_preds_extra).pow(2)
                value_loss_clipped = (returns - values_clipped).pow(2)
                critic_loss_extra = 0.5 * torch.max(value_loss_unclipped, value_loss_clipped).mean()

                self.optimizer_critic.zero_grad()
                critic_loss_extra.backward()
                if self.use_grad_clip:
                    nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)
                self.optimizer_critic.step()
                # æ˜¾å­˜ç¼“å­˜æ¸…ç†
                torch.cuda.empty_cache()

            if self.use_lr_decay and epoch == 0:
                self.scheduler_actor.step()
                self.scheduler_critic.step()

            # æ˜¾å­˜ç¼“å­˜æ¸…ç†
            torch.cuda.empty_cache()

            print(" --------------4-----------------")
            print(f"[æ˜¾å­˜ç›‘æ§] å·²ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MB")
            print(f"        ä¿ç•™ç¼“å­˜: {torch.cuda.memory_reserved() / 1024 ** 2:.2f} MB")
            avg_policy_loss = policy_loss.item()
            avg_q_value = value_preds.mean().item()
            self.ep_avg_losses.append(avg_policy_loss)
            self.ep_avg_qs.append(avg_q_value)

            print(f"âœ… Epoch {self.trajectory_count}: avg_policy_loss: {avg_policy_loss:.6f}, avg_q_value: {avg_q_value:.6f}")

        print(f"ğŸš€ å®Œæˆè½¨è¿¹æ›´æ–°ï¼Œç­–ç•¥ loss: {avg_policy_loss:.6f}ï¼Œå¹³å‡ Q å€¼: {avg_q_value:.6f}")
        self.entropy_coef = self.entropy_coef_max * 0.5 * (
                1 + math.cos(math.pi * self.trajectory_count / self.max_episodes))
        print(f"å½“å‰ entropy_coef: {self.entropy_coef:.6f}")
        self.trajectory_count += 1

    def save(self, filepath):
        torch.save({
            "actor": self.actor.state_dict(),
            "critic": self.critic.state_dict()
        }, filepath)

    def load(self, filepath):
        checkpoint = torch.load(filepath, map_location=self.device)
        if "actor" in checkpoint:
            self.actor.load_state_dict(checkpoint["actor"])
            print("âœ… æˆåŠŸåŠ è½½ actor ç½‘ç»œå‚æ•°")
        else:
            print("âš ï¸ checkpoint ä¸­æ²¡æœ‰ 'actor' é”®ï¼Œè·³è¿‡ actor åŠ è½½")

        if "critic" in checkpoint:
            self.critic.load_state_dict(checkpoint["critic"])
            print("âœ… æˆåŠŸåŠ è½½ critic ç½‘ç»œå‚æ•°")
        else:
            print("âš ï¸ checkpoint ä¸­æ²¡æœ‰ 'critic' é”®ï¼Œè·³è¿‡ critic åŠ è½½")

    def load_actor(self, path):
        actor_state_dict = torch.load(path, map_location=self.device)
        self.actor.load_state_dict(actor_state_dict)

    def resetMemory(self):
        self.actor.reset_memory()
        self.critic.reset_memory()
        print("ğŸ§¿ è®°å¿†åˆå§‹åŒ–: Actor & Critic")

