import torch
import torch.nn as nn
import torch.nn.functional as F
import torch
import os

from PolicyNet import SwinPolicyNet
from ValueNet import CriticNetwork
from T_PolicyNet_ import CNNTransformer
from T_ValueNet import CriticNetwork

class PPO:
    def __init__(self, args):
        self.device = args.device
        self.gamma = args.gamma  # æŠ˜æ‰£å› å­
        self.lamda = args.lamda  # GAEå‚æ•°
        self.epsilon = args.epsilon  # PPO clipèŒƒå›´
        self.K_epochs = args.K_epochs  # ç­–ç•¥æ›´æ–°è½®æ•°
        self.entropy_coef = args.entropy_coef  # ç†µç³»æ•°
        self.clip_value = args.clip_value  # ä»·å€¼å‡½æ•°clipé˜ˆå€¼
        self.max_episodes = args.max_episodes
        self.count = 0

        # Swin ç½‘ç»œåˆå§‹åŒ–
        # self.actor = SwinPolicyNet(
        #     embed_dim=args.embed_dim,
        #     hidden_dim=args.hidden_dim,
        #     dropout_rate=args.dropout_rate
        # ).to(self.device)
        #
        # self.critic = CriticNetwork(
        #     embed_dim=args.embed_dim,
        #     hidden_dim=args.hidden_dim,
        #     dropout_rate=args.dropout_rate
        # ).to(self.device)

        # ç½‘ç»œåˆå§‹åŒ–
        self.actor = CNNTransformer(
            img_feature_dim=args.img_feature_dim,
            transformer_dim=args.transformer_dim,
            hidden_dim=args.hidden_width,
            transformer_heads=args.transformer_heads,
            transformer_layers=args.transformer_layers,
            dropout_rate=args.dropout_rate,
        ).to(self.device)

        self.critic = nn.Sequential(
            CriticNetwork(
                img_feature_dim=args.img_feature_dim,
                transformer_dim=args.transformer_dim,
                hidden_dim=args.hidden_width,
                transformer_heads=args.transformer_heads,
                transformer_layers=args.transformer_layers,
                dropout_rate=args.dropout_rate,
            )
        ).to(self.device)


        # ä¼˜åŒ–å™¨é…ç½®
        self.optimizer = torch.optim.Adam([
            {'params': self.actor.parameters(), 'lr': args.lr_a},
            {'params': self.critic.parameters(), 'lr': args.lr_c}
        ])

        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=args.max_episodes, eta_min=1e-5)

        self.training_log = {
            "policy_loss": [],
            "value_loss": [],
            "entropy": [],
            "actor_grad": [],
            "critic_grad": []
        }
        self.log_file = "training_log.pt"  # âœ… æ—¥å¿—å­˜å‚¨æ–‡ä»¶

    def _save_log(self):
        if len(self.training_log["policy_loss"]) % 100 == 0:
            torch.save(self.training_log, self.log_file)

    def get_action(self, state):
        """ é‡‡æ ·åŠ¨ä½œå¹¶è¿”å›å¤„ç†åçš„PythonåŸç”Ÿç±»å‹ """
        state_tensor = torch.as_tensor(state, device=self.device, dtype=torch.float32)
        with torch.no_grad():
            actions, log_probs = self.actor.sample_action(state_tensor)
        return actions.cpu().numpy().squeeze(0).tolist(), log_probs.cpu().numpy().tolist()

    def update(self, batch_samples):
        """ æ‰¹é‡æ›´æ–°æ ¸å¿ƒé€»è¾‘ """
        # æ˜¾å¼åœ°ä» batch_samples ä¸­åˆ†åˆ« stack åï¼Œå†è½¬æ¢åˆ° GPU
        states = torch.stack([torch.as_tensor(s[0], device=self.device) for s in batch_samples])
        actions = torch.stack([torch.as_tensor(s[1], device=self.device) for s in batch_samples])
        old_log_probs = torch.stack([torch.as_tensor(s[2], device=self.device) for s in batch_samples])
        rewards = torch.stack([torch.as_tensor(s[3], device=self.device) for s in batch_samples])
        next_states = torch.stack([torch.as_tensor(s[4], device=self.device) for s in batch_samples])
        dones = torch.stack([torch.as_tensor(s[5], device=self.device) for s in batch_samples])
        #print(f"âœ… è¾“å…¥åˆ° state çš„ç»´åº¦: {states.shape}")

        # æ£€æŸ¥æ•°æ®æ˜¯å¦éƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        assert states.device == self.device, f"statesè®¾å¤‡ä¸ä¸€è‡´: {states.device} vs {self.device}"

        # ä»·å€¼é¢„æµ‹
        with torch.no_grad():
            values = self.critic(states).squeeze(-1)
            next_values = self.critic(next_states).squeeze(-1)

            # GAEè®¡ç®—
            advantages = torch.zeros_like(rewards)
            gae = 0
            for t in reversed(range(len(rewards))):
                next_value = (1 - dones[t]) * next_values[t].detach()
                delta = rewards[t] + self.gamma * next_value - values[t]
                gae = delta + self.gamma * self.lamda * (1 - dones[t]) * gae
                advantages[t] = gae
            returns = advantages + values

        # ä¼˜åŠ¿å€¼æ ‡å‡†åŒ–
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # è®­ç»ƒ K_epochs è½®
        for _ in range(self.K_epochs):
            self._optimize_epoch(states, actions, old_log_probs, returns, advantages)
        self.scheduler.step()

    def _optimize_epoch(self, states, actions, old_log_probs, returns, advantages):
        """ å•è½®ä¼˜åŒ–æ­¥éª¤ """
        assert states.device == self.device, f"statesè®¾å¤‡ä¸ä¸€è‡´: {states.device} vs {self.device}"
        assert actions.device == self.device, f"actionsè®¾å¤‡ä¸ä¸€è‡´: {actions.device} vs {self.device}"

        # åŠ¨ä½œåˆ†å¸ƒè·å–
        move_dist, jump_dist = self.actor.get_dist(states)

        # æ–°ç­–ç•¥æ¦‚ç‡è®¡ç®—
        new_log_probs = sum([
            1.5 * move_dist.log_prob(actions[:, 0]),
            1.0 * jump_dist.log_prob(actions[:, 1])
        ])

        # PPOç›®æ ‡è®¡ç®—
        ratio = (new_log_probs - old_log_probs).exp()
        clipped_ratio = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon)
        policy_loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()

        # ä»·å€¼å‡½æ•°ä¼˜åŒ–ï¼ˆå¸¦clipï¼‰
        values = self.critic(states).squeeze(-1)
        values_clipped = values + (values - values.detach()).clamp(-self.clip_value, self.clip_value)
        # "Proximal Policy Optimization Algorithms" ç»™å‡ºçš„ ä»·å€¼æŸå¤± (Value Loss) å…¬å¼
        value_loss_unclipped = (values - returns).pow(2)
        value_loss_clipped = (values_clipped - returns).pow(2)
        value_loss = 0.5 * torch.max(value_loss_unclipped, value_loss_clipped).mean()

        # ç†µæ­£åˆ™é¡¹
        entropy = torch.mean(torch.stack([
            dist.entropy().mean()
            for dist in [move_dist, jump_dist]
        ]))

        # æ€»æŸå¤±è®¡ç®—
        total_loss = policy_loss + value_loss + self.entropy_coef * entropy

        # åå‘ä¼ æ’­ä¸æ¢¯åº¦è£å‰ª
        self.optimizer.zero_grad()
        total_loss.backward()
        nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
        nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)
        self.optimizer.step()

        # è®­ç»ƒç›‘æ§
        actor_grad = sum(p.grad.norm().item() for p in self.actor.parameters() if p.grad is not None)
        critic_grad = sum(p.grad.norm().item() for p in self.critic.parameters() if p.grad is not None)

        # âœ… å­˜å‚¨æ•°æ®
        self.training_log["policy_loss"].append(policy_loss.item())
        self.training_log["value_loss"].append(value_loss.item())
        self.training_log["entropy"].append(entropy.item())
        self.training_log["actor_grad"].append(actor_grad)
        self.training_log["critic_grad"].append(critic_grad)

        # âœ… è®­ç»ƒæ—¥å¿—å†™å…¥æ–‡ä»¶
        self._save_log()

        self.count += 1
        print(f"ğŸ’  ç¬¬[{self.count}]æ¬¡ æ›´æ–°......")