import os
import torch
import random
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Categorical
from G_PolicyNet import GTrXLPolicyNet, orthogonal_init
from G_ValueNet import GTrXLValueNet, orthogonal_init

class PPO:
    def __init__(self, args):
        self.device = args.device
        self.gamma = args.gamma
        self.epsilon = args.epsilon
        self.K_epochs = args.K_epochs
        self.entropy_coef = args.entropy_coef
        self.clip_value = args.clip_value
        self.count = 0
        self.use_grad_clip = getattr(args, "use_grad_clip", False)
        self.use_lr_decay = getattr(args, "use_lr_decay", False)

        # æ–°å¢è®°å½•å„ episode æŒ‡æ ‡çš„å±æ€§
        self.ep_avg_losses = []  # ç”¨äºè®°å½•æ¯ä¸ª episode çš„å¹³å‡ç­–ç•¥ lossï¼ˆä¹Ÿå¯åŒ…å« critic lossï¼‰
        self.ep_avg_qs = []      # ç”¨äºè®°å½•æ¯ä¸ª episode çš„å¹³å‡ Q å€¼ï¼ˆCritic è¾“å‡ºå‡å€¼ï¼‰
        self.ep_lengths = []     # è®°å½•æ¯ä¸ª episode çš„æ­¥æ•°ï¼ˆå¦‚æœåœ¨ Worker ä¸­ç»Ÿè®¡ï¼Œå¯ä»¥åœ¨æ­¤æ›´æ–°ï¼‰
        self.ep_rewards = []     # è®°å½•æ¯ä¸ª episode çš„æ€»å¥–åŠ±ï¼ˆå¦‚æœåœ¨ Worker ä¸­ç»Ÿè®¡ï¼Œå¯ä»¥åœ¨æ­¤æ›´æ–°ï¼‰

        # **åˆ›å»º Actor å’Œ Critic**
        self.actor = GTrXLPolicyNet(
            img_feature_dim=args.img_feature_dim,
            transformer_dim=args.transformer_dim,
            hidden_dim=args.hidden_width,
            transformer_heads=args.transformer_heads,
            transformer_layers=args.transformer_layers,
            dropout_rate=args.dropout_rate,
        ).to(self.device)

        self.critic = GTrXLValueNet(
            img_feature_dim=args.img_feature_dim,
            transformer_dim=args.transformer_dim,
            hidden_dim=args.hidden_width,
            transformer_heads=args.transformer_heads,
            transformer_layers=args.transformer_layers,
            dropout_rate=args.dropout_rate,
        ).to(self.device)

        # **ğŸš€ æ‰§è¡Œæ­£äº¤åˆå§‹åŒ–**
        self._initialize_weights()
        self.set_seed(42)

        if getattr(args, "set_adam_eps", False):
            self.optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr=args.lr_a, eps=3e-5)
            self.optimizer_critic = torch.optim.Adam(self.critic.parameters(), lr=args.lr_c, eps=8e-5)
        else:
            self.optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr=args.lr_a)
            self.optimizer_critic = torch.optim.Adam(self.critic.parameters(), lr=args.lr_c)

        self.scheduler_actor = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer_actor, T_max=args.max_train_steps, eta_min=1e-5)
        self.scheduler_critic = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer_critic, T_max=args.max_train_steps, eta_min=3e-5)

        self.checkpoint_path = "checkpoints/Terraria_final_model_3900.pth"
        if os.path.exists(self.checkpoint_path):
            print(f"ğŸ”„ åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹: {self.checkpoint_path}")
            self.load(self.checkpoint_path)
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°å·²ä¿å­˜çš„æ¨¡å‹ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–å‚æ•°")

    def _initialize_weights(self):
        """
        ğŸš€ **ä½¿ç”¨æ­£äº¤åˆå§‹åŒ–å¯¹ Actor å’Œ Critic ç½‘ç»œè¿›è¡Œæƒé‡åˆå§‹åŒ–**
        """
        self.actor.apply(orthogonal_init)
        self.critic.apply(orthogonal_init)
        print("âœ… æ­£äº¤åˆå§‹åŒ–å®Œæˆ: Actor & Critic")

    @staticmethod  # âœ… æ·»åŠ è¿™ä¸ªè£…é¥°å™¨ï¼Œä½¿å…¶æˆä¸ºé™æ€æ–¹æ³•
    def set_seed(seed=42):
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    def get_action(self, state):
        """ é‡‡æ ·åŠ¨ä½œå¹¶è¿”å›å¤„ç†åçš„ Python åŸç”Ÿç±»å‹ """
        state_tensor = torch.as_tensor(state, device=self.device, dtype=torch.float32)
        with torch.no_grad():
            actions, log_probs = self.actor.sample_action(state_tensor)

        return actions.cpu().numpy().squeeze(0).tolist(), log_probs.cpu().numpy().tolist()

    def update(self, replay_buffer):
        """
        æ›´æ–° PPO ç­–ç•¥å’Œ Critic
        replay_buffer ä¸­åŒ…å«ä¸€ä¸ªå•ç‹¬æ ·æœ¬ï¼š (s, a, a_logprob, r, s_, done)
        """
        s, a, a_logprob, r, s_, done = replay_buffer
        s = s.to(self.device)
        a = a.to(self.device)
        a_logprob = a_logprob.to(self.device)
        r = r.to(self.device)
        s_ = s_.to(self.device)
        done = done.to(self.device)

        # è®¡ç®— TD(0) ç›®æ ‡
        with torch.no_grad():
            v_s_ = self.critic(s_, update_memory=False).squeeze(-1)
            v_target = r + self.gamma * (1 - done) * v_s_

        # åˆå§‹åŒ–æŒ‡æ ‡ç´¯åŠ å™¨
        total_policy_loss = 0.0
        total_critic_loss = 0.0
        total_q_value = 0.0

        for epoch in range(self.K_epochs):
            update_memory = (epoch == 0)  # åªæœ‰ç¬¬ä¸€ä¸ª epoch æ›´æ–° memory
            # è°ƒç”¨ actor æ—¶ä¼ é€’ update_memory å‚æ•°
            move_dist, jump_dist = self.actor.get_dist(s, update_memory=update_memory)

            # è°ƒç”¨ critic æ—¶ä¼ é€’ update_memory å‚æ•°
            v_s = self.critic(s, update_memory=update_memory).squeeze(-1)

            advantage = (v_target - v_s).detach()
            # è®¡ç®— PPO æŸå¤±
            ratio = torch.exp((move_dist.log_prob(a[:, 0]) + jump_dist.log_prob(a[:, 1])).view(-1, 1) - a_logprob)
            surr1 = ratio * advantage
            surr2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantage
            policy_loss = -torch.min(surr1, surr2).mean() - self.entropy_coef * (
                        move_dist.entropy().mean() + jump_dist.entropy().mean())

            values_clipped = v_s + (v_s - v_s.detach()).clamp(-self.clip_value, self.clip_value)
            value_loss_unclipped = (v_target - v_s).pow(2)
            value_loss_clipped = (v_target - values_clipped).pow(2)
            critic_loss = 0.5 * torch.max(value_loss_unclipped, value_loss_clipped).mean()

            # æ›´æ–° Actor
            self.optimizer_actor.zero_grad()
            policy_loss.backward()
            if self.use_grad_clip:
                nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
            self.optimizer_actor.step()

            # æ›´æ–° Critic
            self.optimizer_critic.zero_grad()
            critic_loss.backward()
            if self.use_grad_clip:
                nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)
            self.optimizer_critic.step()

            # ç´¯åŠ æŒ‡æ ‡
            total_policy_loss += policy_loss.item()
            total_critic_loss += critic_loss.item()
            total_q_value += v_s.mean().item()

        # è‹¥ä½ åªå…³å¿ƒç­–ç•¥ç½‘ç»œçš„ lossï¼Œåˆ™è®°å½•å¹³å‡ policy loss
        avg_policy_loss = total_policy_loss / self.K_epochs
        avg_critic_loss = total_critic_loss / self.K_epochs
        avg_q_value = total_q_value / self.K_epochs

        self.ep_avg_losses.append(avg_policy_loss)
        self.ep_avg_qs.append(avg_q_value)
        # æ³¨æ„ï¼šep_lengths ä¸ ep_rewards é€šå¸¸ç”±ç¯å¢ƒäº¤äº’ç»Ÿè®¡ï¼Œå¹¶åœ¨ Worker ä¸­è®°å½•åæ›´æ–°åˆ° PPO æ¨¡å‹ä¸­

        if self.use_lr_decay:
            self.scheduler_actor.step()
            self.scheduler_critic.step()

        print(f"ğŸ’  ç¬¬[{self.count}]æ¬¡ æ›´æ–°......")
        self.count += 1

    def save(self, filepath):
        torch.save({
            "actor": self.actor.state_dict(),
            "critic": self.critic.state_dict()
        }, filepath)

    def load(self, filepath):
        checkpoint = torch.load(filepath, map_location=self.device)
        self.actor.load_state_dict(checkpoint["actor"])
        self.critic.load_state_dict(checkpoint["critic"])

    def resetMemory(self):
        self.actor.reset_memory()
        self.critic.reset_memory()
        print("ğŸ§¿ è®°å¿†åˆå§‹åŒ–: Actor & Critic")

