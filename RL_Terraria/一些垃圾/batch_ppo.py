import torch
import torch.nn as nn
import torch.nn.functional as F
from actor_batch import LiteCNNTransformer
from model_critic_batch import CriticNetwork


class PPO:
    def __init__(self, args):
        """ PPO åˆå§‹åŒ– """
        self.device = args.device
        self.gamma = args.gamma
        self.lamda = args.lamda
        self.epsilon = args.epsilon
        self.K_epochs = args.K_epochs
        self.entropy_coef = args.entropy_coef

        # **1ï¸âƒ£ Actor-ç­–ç•¥ç½‘ç»œ**
        self.actor = LiteCNNTransformer(
            img_feature_dim=args.img_feature_dim,
            transformer_dim=args.transformer_dim,
            hidden_dim=args.hidden_width,
            transformer_heads=args.transformer_heads,
            transformer_layers=args.transformer_layers,
            dropout_rate=args.dropout_rate
        ).to(self.device)

        # **2ï¸âƒ£ Critic-ä»·å€¼ç½‘ç»œ**
        self.critic = CriticNetwork(
            img_feature_dim=args.img_feature_dim,
            transformer_dim=args.transformer_dim,
            hidden_dim=args.hidden_width,
            transformer_heads=args.transformer_heads,
            transformer_layers=args.transformer_layers,
            dropout_rate=args.dropout_rate
        ).to(self.device)

        # **3ï¸âƒ£ ä¼˜åŒ–å™¨**
        self.optimizer = torch.optim.Adam([
            {'params': self.actor.parameters(), 'lr': args.lr_a},
            {'params': self.critic.parameters(), 'lr': args.lr_c}
        ])

    def get_action(self, state):
        """ ğŸ¯ è·å–å•ä¸ªåŠ¨ä½œ """
        if not isinstance(state, torch.Tensor):
            state = torch.tensor(state, dtype=torch.float32, device=self.device)

        with torch.no_grad():
            actions, log_probs = self.actor.sample_action(state)

        actions = actions.cpu().numpy().squeeze(0).tolist()  # âœ… ç¡®ä¿è¿”å› `[move, jump, down]`
        log_probs = log_probs.cpu().numpy().tolist()

        return actions, log_probs

    def update(self, batch_samples):
        """ âœ… **æ‰¹é‡æ›´æ–°ï¼ˆ16 ä¸ªæ ·æœ¬ä¸€èµ·æ›´æ–°ï¼Œä½¿ç”¨ GAEï¼‰** """
        print("------ å¼€å§‹æ‰¹é‡æ›´æ–° --------")

        # **è½¬æ¢ batch æ•°æ®**
        states = torch.stack([s[0] for s in batch_samples]).to(self.device)  # [16, 8, 3, 224, 224]
        actions = torch.stack([s[1] for s in batch_samples]).to(self.device)  # [16, 3]
        old_log_probs = torch.stack([s[2] for s in batch_samples]).to(self.device)  # [16]
        rewards = torch.stack([s[3] for s in batch_samples]).to(self.device)  # [16]
        next_states = torch.stack([s[4] for s in batch_samples]).to(self.device)  # [16, 8, 3, 224, 224]
        dones = torch.stack([s[5] for s in batch_samples]).to(self.device)  # [16]

        # **è®¡ç®— Critic é¢„æµ‹çš„å€¼**
        with torch.no_grad():
            values = self.critic(states).squeeze(-1)  # [16]
            next_values = self.critic(next_states).squeeze(-1)  # [16]

            # **âœ… ä½¿ç”¨ GAE è®¡ç®— Advantage**
            advantages = torch.zeros_like(rewards).to(self.device)
            gae = 0  # åˆå§‹åŒ– GAE å€¼
            for t in reversed(range(len(rewards))):
                delta = rewards[t] + self.gamma * (1 - dones[t]) * next_values[t] - values[t]
                gae = delta + self.gamma * self.lamda * (1 - dones[t]) * gae  # âœ… GAE é€’æ¨è®¡ç®—
                advantages[t] = gae

            returns = advantages + values  # è®¡ç®—ç›®æ ‡ Q å€¼

        # **ç­–ç•¥ä¼˜åŒ–**
        for _ in range(self.K_epochs):
            self._optimize(states, actions, old_log_probs, returns, advantages)

    def _optimize(self, states, actions, old_log_probs, returns, advantages):
        """ âœ… **æ‰¹é‡æ¢¯åº¦æ›´æ–°** """
        move_dists, jump_dists, down_dists = self.actor.get_dist(states)

        # **è®¡ç®—æ–° log_prob**
        new_log_probs = (
            move_dists.log_prob(actions[:, 0]) +
            jump_dists.log_prob(actions[:, 1]) +
            down_dists.log_prob(actions[:, 2])
        )  # [16]

        # **PPO ç›®æ ‡**
        ratio = torch.exp(new_log_probs - old_log_probs)  # [16]
        surr1 = ratio * advantages  # [16]
        surr2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantages  # [16]
        policy_loss = -torch.min(surr1, surr2).mean()  # âœ… è®¡ç®—å‡å€¼

        # **ä»·å€¼æŸå¤±**
        values = self.critic(states).squeeze(-1)  # [16]
        value_loss = F.mse_loss(returns, values)  # âœ… è®¡ç®—å‡æ–¹è¯¯å·®

        # **ç†µæ­£åˆ™åŒ–**
        entropy = sum(d.entropy().mean() for d in [move_dists, jump_dists, down_dists])

        # **ä¼˜åŒ–**
        loss = policy_loss + 0.5 * value_loss - self.entropy_coef * entropy
        self.optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
        nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)
        self.optimizer.step()

        # **æ‰“å°æ—¥å¿—**
        print(f"ç­–ç•¥æŸå¤±: {policy_loss.item():.4f}, ä»·å€¼æŸå¤±: {value_loss.item():.4f}, ç†µ: {entropy.item():.4f}")
