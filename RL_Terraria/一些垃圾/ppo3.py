import torch
import torch.nn as nn
import torch.nn.functional as F
from model4 import LiteCNNTransformer
from model_critic import CriticNetwork



class PPO:
    def __init__(self, args):
        """ PPO åˆå§‹åŒ– """
        self.device = args.device
        self.gamma = args.gamma
        self.lamda = args.lamda
        self.epsilon = args.epsilon
        self.K_epochs = args.K_epochs
        self.entropy_coef = args.entropy_coef

        # **1ï¸âƒ£ Actor-ç­–ç•¥ç½‘ç»œ**
        self.actor = LiteCNNTransformer(
            img_feature_dim=args.img_feature_dim,
            transformer_dim=args.transformer_dim,
            hidden_dim=args.hidden_width,
            transformer_heads=args.transformer_heads,
            transformer_layers=args.transformer_layers,
            dropout_rate=args.dropout_rate
        ).to(self.device)

        # **2ï¸âƒ£ Critic-ä»·å€¼ç½‘ç»œ**
        self.critic = CriticNetwork(
            img_feature_dim=args.img_feature_dim,
            transformer_dim=args.transformer_dim,
            hidden_dim=args.hidden_width,
            transformer_heads=args.transformer_heads,
            transformer_layers=args.transformer_layers,
            dropout_rate=args.dropout_rate
        ).to(self.device)

        # **3ï¸âƒ£ ä¼˜åŒ–å™¨**
        self.optimizer = torch.optim.Adam([
            {'params': self.actor.parameters(), 'lr': args.lr_a},
            {'params': self.critic.parameters(), 'lr': args.lr_c}
        ])

    def get_action(self, state):
        """ ğŸ¯ è·å–å•ä¸ªåŠ¨ä½œ """
        if not isinstance(state, torch.Tensor):
            state = torch.tensor(state, dtype=torch.float32, device=self.device)

        with torch.no_grad():
            actions, log_probs = self.actor.sample_action(state)

        return actions.cpu().numpy(), log_probs.cpu().numpy()

    def update(self, sample):
        """ âœ… å•æ ·æœ¬æ›´æ–° """
        print("------ å¼€å§‹æ›´æ–° --------")
        state, action, old_log_prob, reward, next_state, done = sample

        # **ç¡®ä¿æ•°æ®æ˜¯ `[8, 3, 224, 224]`ï¼Œä¸é¢å¤–å¢åŠ  batch ç»´åº¦**
        state = state.to(self.device)  # [8, 3, 224, 224]
        next_state = next_state.to(self.device)
        action = action.to(self.device)  # [3]
        old_log_prob = old_log_prob.to(self.device)  # æ ‡é‡
        reward = reward.to(self.device)  # æ ‡é‡
        done = done.to(self.device)  # æ ‡é‡

        # **è®¡ç®— Critic é¢„æµ‹çš„å€¼**
        with torch.no_grad():
            value = self.critic(state).squeeze(-1)
            next_value = self.critic(next_state).squeeze(-1)

            # **è®¡ç®— Advantage**
            advantage = reward + self.gamma * (1 - done) * next_value - value
            return_val = advantage + value  # ç›®æ ‡ Q å€¼

        # **ç­–ç•¥ä¼˜åŒ–**
        for _ in range(self.K_epochs):
            self._optimize(state, action, old_log_prob, return_val, advantage)

    def _optimize(self, state, action, old_log_prob, return_val, advantage):
        """ âœ… å•æ ·æœ¬æ¢¯åº¦æ›´æ–° """
        move_dist, jump_dist, down_dist = self.actor.get_dist(state)

        new_log_prob = (
            move_dist.log_prob(action[0]) +
            jump_dist.log_prob(action[1]) +
            down_dist.log_prob(action[2])
        )

        # **PPO ç›®æ ‡**
        ratio = torch.exp(new_log_prob - old_log_prob)
        surr1 = ratio * advantage
        surr2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantage
        policy_loss = -torch.min(surr1, surr2)

        # **ä»·å€¼æŸå¤±**
        value = self.critic(state).squeeze(-1)  # è®¡ç®— V(s)
        value_loss = F.mse_loss(return_val, value)

        # **ç†µæ­£åˆ™åŒ–**
        entropy = sum(d.entropy() for d in [move_dist, jump_dist, down_dist])

        # **ä¼˜åŒ–**
        loss = policy_loss + 0.5 * value_loss - self.entropy_coef * entropy
        self.optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
        nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)
        self.optimizer.step()

        # **æ‰“å°æ—¥å¿—**
        print(f"ç­–ç•¥æŸå¤±: {policy_loss.item():.4f}, ä»·å€¼æŸå¤±: {value_loss.item():.4f}, ç†µ: {entropy.item():.4f}")
