import torch
import torch.nn as nn
import torch.nn.functional as F
from Categorical_policy_model import LiteCNNTransformer
import numpy as np


class PPO:
    def __init__(self, args):
        # è¶…å‚æ•°é…ç½®
        self.device = args.device
        self.gamma = args.gamma
        self.lamda = args.lamda
        self.epsilon = args.epsilon
        self.K_epochs = args.K_epochs
        self.entropy_coef = args.entropy_coef
        self.batch_size = args.batch_size
        self.mini_batch_size = args.mini_batch_size

        # ç½‘ç»œåˆå§‹åŒ–
        self.actor = LiteCNNTransformer(
            img_feature_dim=args.img_feature_dim,
            transformer_dim=args.transformer_dim,
            hidden_dim=args.hidden_width,
            transformer_heads=args.transformer_heads,
            transformer_layers=args.transformer_layers,
            dropout_rate=args.dropout_rate,
            max_seq_len=args.seq_len
        ).to(self.device)

        self.critic = nn.Sequential(
            LiteCNNTransformer(
                img_feature_dim=args.img_feature_dim,
                transformer_dim=args.transformer_dim,
                hidden_dim=args.hidden_width,
                transformer_heads=args.transformer_heads,
                transformer_layers=args.transformer_layers,
                dropout_rate=args.dropout_rate,
                max_seq_len=args.seq_len
            ),
            nn.Linear(args.hidden_width, 1)
        ).to(self.device)

        # ä¼˜åŒ–å™¨é…ç½®
        self.optimizer = torch.optim.Adam([
            {'params': self.actor.parameters(), 'lr': args.lr_a},
            {'params': self.critic.parameters(), 'lr': args.lr_c}
        ])

    def get_action(self, state):
        """è·å–åŠ¨ä½œ"""
        if isinstance(state, np.ndarray):
            state = torch.FloatTensor(state).to(self.device)
        if state.ndim == 4:
            state = state.unsqueeze(1)  # æ·»åŠ åºåˆ—ç»´åº¦

        with torch.no_grad():
            actions, log_probs = self.actor.sample_action(state)  # âœ… ç›´æ¥è°ƒç”¨ sample_action()

        return actions.cpu().numpy(), log_probs.cpu().numpy()

    def update(self, buffer):
        """æ ¸å¿ƒä¼˜åŒ–é€»è¾‘"""
        states, actions, old_log_probs, rewards, next_states, dones = buffer
        #obs, action, log_prob, reward, next_obs, done
        # æ•°æ®é¢„å¤„ç†
        states = self._format_input(states)
        print(f"ğŸš€ å¤„ç†åçš„ states å½¢çŠ¶: {states.shape}")
        next_states = self._format_input(next_states)
        actions = torch.LongTensor(actions).to(self.device)

        # GAEè®¡ç®—
        with torch.no_grad():
            values = self.critic(states)
            next_values = self.critic(next_states)
            deltas = rewards + self.gamma * (1 - dones) * next_values - values
            advantages = self._compute_gae(deltas, dones)
            returns = advantages + values

        # ç­–ç•¥ä¼˜åŒ–
        for _ in range(self.K_epochs):
            for indices in self._generate_batches():
                self._optimize(
                    states[indices],
                    actions[indices],
                    old_log_probs[indices],
                    returns[indices],
                    advantages[indices]
                )

    def _format_input(self, x):
        """ç»Ÿä¸€è¾“å…¥æ ¼å¼ [B, T, C, H, W]"""
        x = torch.as_tensor(x, device=self.device, dtype=torch.float32)
        if x.ndim == 4:  # è¡¥å……åºåˆ—ç»´åº¦
            x = x.unsqueeze(1).expand(-1, self.actor.seq_len, -1, -1, -1)
        return x

    def _compute_gae(self, deltas, dones):
        """å‘é‡åŒ–GAEè®¡ç®—"""
        advantages = torch.zeros_like(deltas)
        gae = 0.0
        for t in reversed(range(len(deltas))):
            gae = deltas[t] + self.gamma * self.lamda * (1 - dones[t]) * gae
            advantages[t] = gae
        return (advantages - advantages.mean()) / (advantages.std() + 1e-5)

    def _generate_batches(self):
        """ç”Ÿæˆéšæœºmini-batch"""
        perm = torch.randperm(self.batch_size)
        for i in range(0, self.batch_size, self.mini_batch_size):
            yield perm[i:i + self.mini_batch_size]

    def _optimize(self, states, actions, old_log_probs, returns, advantages):
        """ç­–ç•¥å’Œä»·å€¼ç½‘ç»œè”åˆä¼˜åŒ–"""
        # ç­–ç•¥æŸå¤±
        move_dist, jump_dist, down_dist = self.actor.get_dist(states)
        new_log_probs = (
                move_dist.log_prob(actions[..., 0]) +
                jump_dist.log_prob(actions[..., 1]) +
                down_dist.log_prob(actions[..., 2])
        )

        ratios = (new_log_probs - old_log_probs).exp()
        surr1 = ratios * advantages
        surr2 = torch.clamp(ratios, 1 - self.epsilon, 1 + self.epsilon) * advantages
        policy_loss = -torch.min(surr1, surr2).mean()

        # ä»·å€¼æŸå¤±
        values = self.critic(states)
        value_loss = F.mse_loss(returns, values)

        # ç†µæ­£åˆ™åŒ–
        entropy = sum(d.entropy().mean() for d in [move_dist, jump_dist, down_dist])

        # è”åˆä¼˜åŒ–
        loss = policy_loss + 0.5 * value_loss - self.entropy_coef * entropy
        self.optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
        nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)
        self.optimizer.step()